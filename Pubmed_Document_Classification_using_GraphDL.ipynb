{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1op-CbyLuN4",
    "outputId": "91e66a67-c52e-442b-a766-d86996856537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 7.9 MB 5.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 6.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 407 kB 7.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 41 kB 774 kB/s \n",
      "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dszt2RUHE7lW"
   },
   "source": [
    "# Node Classification with Graph Neural Networks\n",
    "\n",
    "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
    "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
    "\n",
    "To demonstrate, we make use of the `Pubmed` dataset, which is a **citation network** where nodes represent documents.\n",
    "Each node is described by a 500-dimensional bag-of-words feature vector.\n",
    "Two documents are connected if there exists a citation link between them.\n",
    "The task is to infer the category of each document (3 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imGrKO5YH11-",
    "outputId": "37b771fc-1431-4bf6-a681-0dd166e4887f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Pubmed():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "\n",
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
      "===========================================================================================================\n",
      "Number of nodes: 19717\n",
      "Number of edges: 88648\n",
      "Average node degree: 4.50\n",
      "Number of training nodes: 60\n",
      "Training node label rate: 0.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Pubmed', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OWGw54wRd98"
   },
   "source": [
    "## Training a Graph Convolution Network (GCN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmXWs1dKIzD8",
    "outputId": "8c9f5b03-f834-4941-e718-f1231d37c79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(500, 16)\n",
      "  (conv2): GCNConv(16, 3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpdscco5g6kG"
   },
   "source": [
    "Let's train our model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "p3TAi69zI1bO",
    "outputId": "bff1e713-a000-4710-f649-1033b2c23c69"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.0991\n",
      "Epoch: 002, Loss: 1.0922\n",
      "Epoch: 003, Loss: 1.0865\n",
      "Epoch: 004, Loss: 1.0775\n",
      "Epoch: 005, Loss: 1.0687\n",
      "Epoch: 006, Loss: 1.0552\n",
      "Epoch: 007, Loss: 1.0541\n",
      "Epoch: 008, Loss: 1.0433\n",
      "Epoch: 009, Loss: 1.0276\n",
      "Epoch: 010, Loss: 1.0165\n",
      "Epoch: 011, Loss: 1.0102\n",
      "Epoch: 012, Loss: 0.9949\n",
      "Epoch: 013, Loss: 0.9852\n",
      "Epoch: 014, Loss: 0.9748\n",
      "Epoch: 015, Loss: 0.9533\n",
      "Epoch: 016, Loss: 0.9447\n",
      "Epoch: 017, Loss: 0.9204\n",
      "Epoch: 018, Loss: 0.9012\n",
      "Epoch: 019, Loss: 0.9108\n",
      "Epoch: 020, Loss: 0.9059\n",
      "Epoch: 021, Loss: 0.8768\n",
      "Epoch: 022, Loss: 0.8435\n",
      "Epoch: 023, Loss: 0.8474\n",
      "Epoch: 024, Loss: 0.8093\n",
      "Epoch: 025, Loss: 0.7907\n",
      "Epoch: 026, Loss: 0.8050\n",
      "Epoch: 027, Loss: 0.8111\n",
      "Epoch: 028, Loss: 0.7745\n",
      "Epoch: 029, Loss: 0.7735\n",
      "Epoch: 030, Loss: 0.7507\n",
      "Epoch: 031, Loss: 0.7315\n",
      "Epoch: 032, Loss: 0.7332\n",
      "Epoch: 033, Loss: 0.7078\n",
      "Epoch: 034, Loss: 0.6625\n",
      "Epoch: 035, Loss: 0.7063\n",
      "Epoch: 036, Loss: 0.6926\n",
      "Epoch: 037, Loss: 0.6372\n",
      "Epoch: 038, Loss: 0.6071\n",
      "Epoch: 039, Loss: 0.6383\n",
      "Epoch: 040, Loss: 0.6324\n",
      "Epoch: 041, Loss: 0.5956\n",
      "Epoch: 042, Loss: 0.6262\n",
      "Epoch: 043, Loss: 0.5832\n",
      "Epoch: 044, Loss: 0.5711\n",
      "Epoch: 045, Loss: 0.5736\n",
      "Epoch: 046, Loss: 0.5387\n",
      "Epoch: 047, Loss: 0.5195\n",
      "Epoch: 048, Loss: 0.5488\n",
      "Epoch: 049, Loss: 0.5537\n",
      "Epoch: 050, Loss: 0.5284\n",
      "Epoch: 051, Loss: 0.4637\n",
      "Epoch: 052, Loss: 0.4994\n",
      "Epoch: 053, Loss: 0.4875\n",
      "Epoch: 054, Loss: 0.4652\n",
      "Epoch: 055, Loss: 0.4646\n",
      "Epoch: 056, Loss: 0.4831\n",
      "Epoch: 057, Loss: 0.4349\n",
      "Epoch: 058, Loss: 0.4198\n",
      "Epoch: 059, Loss: 0.4439\n",
      "Epoch: 060, Loss: 0.4504\n",
      "Epoch: 061, Loss: 0.4097\n",
      "Epoch: 062, Loss: 0.4387\n",
      "Epoch: 063, Loss: 0.3843\n",
      "Epoch: 064, Loss: 0.4114\n",
      "Epoch: 065, Loss: 0.4096\n",
      "Epoch: 066, Loss: 0.3961\n",
      "Epoch: 067, Loss: 0.3651\n",
      "Epoch: 068, Loss: 0.3645\n",
      "Epoch: 069, Loss: 0.3983\n",
      "Epoch: 070, Loss: 0.3899\n",
      "Epoch: 071, Loss: 0.3969\n",
      "Epoch: 072, Loss: 0.3424\n",
      "Epoch: 073, Loss: 0.3744\n",
      "Epoch: 074, Loss: 0.3705\n",
      "Epoch: 075, Loss: 0.3268\n",
      "Epoch: 076, Loss: 0.3992\n",
      "Epoch: 077, Loss: 0.3461\n",
      "Epoch: 078, Loss: 0.3258\n",
      "Epoch: 079, Loss: 0.3094\n",
      "Epoch: 080, Loss: 0.3516\n",
      "Epoch: 081, Loss: 0.3634\n",
      "Epoch: 082, Loss: 0.3248\n",
      "Epoch: 083, Loss: 0.2929\n",
      "Epoch: 084, Loss: 0.3228\n",
      "Epoch: 085, Loss: 0.3197\n",
      "Epoch: 086, Loss: 0.3379\n",
      "Epoch: 087, Loss: 0.2979\n",
      "Epoch: 088, Loss: 0.2786\n",
      "Epoch: 089, Loss: 0.3101\n",
      "Epoch: 090, Loss: 0.3134\n",
      "Epoch: 091, Loss: 0.2749\n",
      "Epoch: 092, Loss: 0.3057\n",
      "Epoch: 093, Loss: 0.2899\n",
      "Epoch: 094, Loss: 0.2963\n",
      "Epoch: 095, Loss: 0.3272\n",
      "Epoch: 096, Loss: 0.2697\n",
      "Epoch: 097, Loss: 0.3080\n",
      "Epoch: 098, Loss: 0.2830\n",
      "Epoch: 099, Loss: 0.2864\n",
      "Epoch: 100, Loss: 0.3069\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opBBGQHqg5ZO"
   },
   "source": [
    "After training the model, we can check its test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zOh6IIeI3Op",
    "outputId": "aabe9ca6-f904-45ea-d793-c4bcfc040a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7850\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lfe7xQMyIpdG"
   },
   "source": [
    "## Training a Graph Attention Network (GAT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af7Ul8N9I8uk",
    "outputId": "65b1a798-901f-45f7-cce7-ad1fb3aea684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(500, 8, heads=8)\n",
      "  (conv2): GATConv(64, 3, heads=8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(heads*hidden_channels, dataset.num_classes,concat=False, heads=heads,dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GAT(hidden_channels=8, heads=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "o0-U-QYJJMJD",
    "outputId": "1e5171bc-7fa1-4abf-e1d5-63f2b02f8bdd"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.1005, Val: 0.3620, Test: 0.3550\n",
      "Epoch: 002, Loss: 1.0936, Val: 0.5480, Test: 0.5440\n",
      "Epoch: 003, Loss: 1.0957, Val: 0.7480, Test: 0.7230\n",
      "Epoch: 004, Loss: 1.0913, Val: 0.7600, Test: 0.7330\n",
      "Epoch: 005, Loss: 1.0885, Val: 0.7500, Test: 0.7290\n",
      "Epoch: 006, Loss: 1.0882, Val: 0.7560, Test: 0.7330\n",
      "Epoch: 007, Loss: 1.0861, Val: 0.7640, Test: 0.7300\n",
      "Epoch: 008, Loss: 1.0813, Val: 0.7620, Test: 0.7080\n",
      "Epoch: 009, Loss: 1.0769, Val: 0.7000, Test: 0.6650\n",
      "Epoch: 010, Loss: 1.0781, Val: 0.6860, Test: 0.6610\n",
      "Epoch: 011, Loss: 1.0732, Val: 0.7080, Test: 0.6850\n",
      "Epoch: 012, Loss: 1.0697, Val: 0.7300, Test: 0.7040\n",
      "Epoch: 013, Loss: 1.0651, Val: 0.7540, Test: 0.7180\n",
      "Epoch: 014, Loss: 1.0611, Val: 0.7540, Test: 0.7180\n",
      "Epoch: 015, Loss: 1.0550, Val: 0.7440, Test: 0.7160\n",
      "Epoch: 016, Loss: 1.0493, Val: 0.7380, Test: 0.7160\n",
      "Epoch: 017, Loss: 1.0482, Val: 0.7420, Test: 0.7160\n",
      "Epoch: 018, Loss: 1.0469, Val: 0.7380, Test: 0.7110\n",
      "Epoch: 019, Loss: 1.0390, Val: 0.7340, Test: 0.7040\n",
      "Epoch: 020, Loss: 1.0288, Val: 0.7400, Test: 0.7020\n",
      "Epoch: 021, Loss: 1.0193, Val: 0.7460, Test: 0.7030\n",
      "Epoch: 022, Loss: 1.0238, Val: 0.7440, Test: 0.7020\n",
      "Epoch: 023, Loss: 1.0029, Val: 0.7440, Test: 0.7050\n",
      "Epoch: 024, Loss: 0.9965, Val: 0.7400, Test: 0.7050\n",
      "Epoch: 025, Loss: 0.9903, Val: 0.7400, Test: 0.7070\n",
      "Epoch: 026, Loss: 0.9793, Val: 0.7440, Test: 0.7070\n",
      "Epoch: 027, Loss: 0.9723, Val: 0.7480, Test: 0.7090\n",
      "Epoch: 028, Loss: 0.9646, Val: 0.7520, Test: 0.7120\n",
      "Epoch: 029, Loss: 0.9642, Val: 0.7520, Test: 0.7110\n",
      "Epoch: 030, Loss: 0.9258, Val: 0.7520, Test: 0.7130\n",
      "Epoch: 031, Loss: 0.9268, Val: 0.7400, Test: 0.7070\n",
      "Epoch: 032, Loss: 0.9161, Val: 0.7380, Test: 0.7070\n",
      "Epoch: 033, Loss: 0.9053, Val: 0.7380, Test: 0.7080\n",
      "Epoch: 034, Loss: 0.8969, Val: 0.7380, Test: 0.7060\n",
      "Epoch: 035, Loss: 0.8953, Val: 0.7400, Test: 0.7070\n",
      "Epoch: 036, Loss: 0.8844, Val: 0.7360, Test: 0.7080\n",
      "Epoch: 037, Loss: 0.8637, Val: 0.7440, Test: 0.7100\n",
      "Epoch: 038, Loss: 0.8546, Val: 0.7480, Test: 0.7120\n",
      "Epoch: 039, Loss: 0.8216, Val: 0.7500, Test: 0.7090\n",
      "Epoch: 040, Loss: 0.8256, Val: 0.7500, Test: 0.7090\n",
      "Epoch: 041, Loss: 0.8025, Val: 0.7500, Test: 0.7100\n",
      "Epoch: 042, Loss: 0.8363, Val: 0.7500, Test: 0.7110\n",
      "Epoch: 043, Loss: 0.7865, Val: 0.7500, Test: 0.7110\n",
      "Epoch: 044, Loss: 0.7688, Val: 0.7580, Test: 0.7120\n",
      "Epoch: 045, Loss: 0.7826, Val: 0.7600, Test: 0.7110\n",
      "Epoch: 046, Loss: 0.7560, Val: 0.7580, Test: 0.7110\n",
      "Epoch: 047, Loss: 0.6967, Val: 0.7600, Test: 0.7120\n",
      "Epoch: 048, Loss: 0.7324, Val: 0.7600, Test: 0.7160\n",
      "Epoch: 049, Loss: 0.7020, Val: 0.7560, Test: 0.7130\n",
      "Epoch: 050, Loss: 0.6997, Val: 0.7560, Test: 0.7140\n",
      "Epoch: 051, Loss: 0.6725, Val: 0.7480, Test: 0.7180\n",
      "Epoch: 052, Loss: 0.6685, Val: 0.7520, Test: 0.7170\n",
      "Epoch: 053, Loss: 0.6607, Val: 0.7520, Test: 0.7180\n",
      "Epoch: 054, Loss: 0.6055, Val: 0.7520, Test: 0.7180\n",
      "Epoch: 055, Loss: 0.6246, Val: 0.7520, Test: 0.7160\n",
      "Epoch: 056, Loss: 0.6231, Val: 0.7540, Test: 0.7220\n",
      "Epoch: 057, Loss: 0.5914, Val: 0.7500, Test: 0.7270\n",
      "Epoch: 058, Loss: 0.6145, Val: 0.7500, Test: 0.7320\n",
      "Epoch: 059, Loss: 0.5926, Val: 0.7500, Test: 0.7340\n",
      "Epoch: 060, Loss: 0.5604, Val: 0.7500, Test: 0.7350\n",
      "Epoch: 061, Loss: 0.5375, Val: 0.7500, Test: 0.7370\n",
      "Epoch: 062, Loss: 0.5245, Val: 0.7520, Test: 0.7390\n",
      "Epoch: 063, Loss: 0.5241, Val: 0.7580, Test: 0.7330\n",
      "Epoch: 064, Loss: 0.5023, Val: 0.7640, Test: 0.7310\n",
      "Epoch: 065, Loss: 0.4988, Val: 0.7660, Test: 0.7310\n",
      "Epoch: 066, Loss: 0.4944, Val: 0.7660, Test: 0.7360\n",
      "Epoch: 067, Loss: 0.5270, Val: 0.7660, Test: 0.7360\n",
      "Epoch: 068, Loss: 0.4997, Val: 0.7660, Test: 0.7390\n",
      "Epoch: 069, Loss: 0.4896, Val: 0.7640, Test: 0.7410\n",
      "Epoch: 070, Loss: 0.4758, Val: 0.7660, Test: 0.7410\n",
      "Epoch: 071, Loss: 0.5029, Val: 0.7680, Test: 0.7410\n",
      "Epoch: 072, Loss: 0.4917, Val: 0.7740, Test: 0.7440\n",
      "Epoch: 073, Loss: 0.4045, Val: 0.7760, Test: 0.7460\n",
      "Epoch: 074, Loss: 0.4079, Val: 0.7760, Test: 0.7420\n",
      "Epoch: 075, Loss: 0.4408, Val: 0.7800, Test: 0.7470\n",
      "Epoch: 076, Loss: 0.4293, Val: 0.7820, Test: 0.7480\n",
      "Epoch: 077, Loss: 0.4405, Val: 0.7820, Test: 0.7470\n",
      "Epoch: 078, Loss: 0.4125, Val: 0.7800, Test: 0.7490\n",
      "Epoch: 079, Loss: 0.3754, Val: 0.7800, Test: 0.7480\n",
      "Epoch: 080, Loss: 0.4252, Val: 0.7800, Test: 0.7530\n",
      "Epoch: 081, Loss: 0.4050, Val: 0.7800, Test: 0.7540\n",
      "Epoch: 082, Loss: 0.3493, Val: 0.7840, Test: 0.7550\n",
      "Epoch: 083, Loss: 0.3518, Val: 0.7820, Test: 0.7540\n",
      "Epoch: 084, Loss: 0.3787, Val: 0.7800, Test: 0.7550\n",
      "Epoch: 085, Loss: 0.3724, Val: 0.7760, Test: 0.7620\n",
      "Epoch: 086, Loss: 0.3945, Val: 0.7780, Test: 0.7630\n",
      "Epoch: 087, Loss: 0.3786, Val: 0.7820, Test: 0.7640\n",
      "Epoch: 088, Loss: 0.3675, Val: 0.7820, Test: 0.7650\n",
      "Epoch: 089, Loss: 0.3589, Val: 0.7880, Test: 0.7630\n",
      "Epoch: 090, Loss: 0.3380, Val: 0.7880, Test: 0.7620\n",
      "Epoch: 091, Loss: 0.3759, Val: 0.7920, Test: 0.7640\n",
      "Epoch: 092, Loss: 0.3327, Val: 0.7920, Test: 0.7650\n",
      "Epoch: 093, Loss: 0.3397, Val: 0.7900, Test: 0.7680\n",
      "Epoch: 094, Loss: 0.3220, Val: 0.7860, Test: 0.7690\n",
      "Epoch: 095, Loss: 0.3286, Val: 0.7860, Test: 0.7660\n",
      "Epoch: 096, Loss: 0.3043, Val: 0.7840, Test: 0.7710\n",
      "Epoch: 097, Loss: 0.3441, Val: 0.7800, Test: 0.7720\n",
      "Epoch: 098, Loss: 0.3496, Val: 0.7860, Test: 0.7700\n",
      "Epoch: 099, Loss: 0.2752, Val: 0.7780, Test: 0.7690\n",
      "Epoch: 100, Loss: 0.3188, Val: 0.7800, Test: 0.7710\n",
      "Epoch: 101, Loss: 0.3309, Val: 0.7860, Test: 0.7730\n",
      "Epoch: 102, Loss: 0.3318, Val: 0.7860, Test: 0.7740\n",
      "Epoch: 103, Loss: 0.3123, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 104, Loss: 0.3144, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 105, Loss: 0.3087, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 106, Loss: 0.2875, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 107, Loss: 0.2649, Val: 0.7860, Test: 0.7770\n",
      "Epoch: 108, Loss: 0.2895, Val: 0.7840, Test: 0.7770\n",
      "Epoch: 109, Loss: 0.3025, Val: 0.7860, Test: 0.7750\n",
      "Epoch: 110, Loss: 0.3034, Val: 0.7820, Test: 0.7760\n",
      "Epoch: 111, Loss: 0.2885, Val: 0.7780, Test: 0.7780\n",
      "Epoch: 112, Loss: 0.3371, Val: 0.7820, Test: 0.7770\n",
      "Epoch: 113, Loss: 0.3056, Val: 0.7840, Test: 0.7760\n",
      "Epoch: 114, Loss: 0.3304, Val: 0.7860, Test: 0.7730\n",
      "Epoch: 115, Loss: 0.2658, Val: 0.7940, Test: 0.7760\n",
      "Epoch: 116, Loss: 0.2708, Val: 0.7940, Test: 0.7750\n",
      "Epoch: 117, Loss: 0.3050, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 118, Loss: 0.3034, Val: 0.7960, Test: 0.7810\n",
      "Epoch: 119, Loss: 0.2905, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 120, Loss: 0.2587, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 121, Loss: 0.2633, Val: 0.7940, Test: 0.7810\n",
      "Epoch: 122, Loss: 0.2533, Val: 0.7980, Test: 0.7820\n",
      "Epoch: 123, Loss: 0.2878, Val: 0.8000, Test: 0.7820\n",
      "Epoch: 124, Loss: 0.2493, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 125, Loss: 0.1981, Val: 0.7920, Test: 0.7770\n",
      "Epoch: 126, Loss: 0.2532, Val: 0.7860, Test: 0.7800\n",
      "Epoch: 127, Loss: 0.2813, Val: 0.7860, Test: 0.7790\n",
      "Epoch: 128, Loss: 0.2650, Val: 0.7860, Test: 0.7780\n",
      "Epoch: 129, Loss: 0.2699, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 130, Loss: 0.2487, Val: 0.7940, Test: 0.7750\n",
      "Epoch: 131, Loss: 0.2505, Val: 0.7940, Test: 0.7730\n",
      "Epoch: 132, Loss: 0.2327, Val: 0.7940, Test: 0.7770\n",
      "Epoch: 133, Loss: 0.2599, Val: 0.7940, Test: 0.7750\n",
      "Epoch: 134, Loss: 0.2744, Val: 0.7960, Test: 0.7760\n",
      "Epoch: 135, Loss: 0.2321, Val: 0.7960, Test: 0.7780\n",
      "Epoch: 136, Loss: 0.2127, Val: 0.7940, Test: 0.7770\n",
      "Epoch: 137, Loss: 0.2568, Val: 0.7940, Test: 0.7800\n",
      "Epoch: 138, Loss: 0.2116, Val: 0.7940, Test: 0.7810\n",
      "Epoch: 139, Loss: 0.2131, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 140, Loss: 0.2522, Val: 0.7900, Test: 0.7790\n",
      "Epoch: 141, Loss: 0.2080, Val: 0.7880, Test: 0.7850\n",
      "Epoch: 142, Loss: 0.2492, Val: 0.7880, Test: 0.7830\n",
      "Epoch: 143, Loss: 0.2676, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 144, Loss: 0.2381, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 145, Loss: 0.2267, Val: 0.7900, Test: 0.7810\n",
      "Epoch: 146, Loss: 0.2687, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 147, Loss: 0.2401, Val: 0.7920, Test: 0.7770\n",
      "Epoch: 148, Loss: 0.2338, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 149, Loss: 0.1962, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 150, Loss: 0.2463, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 151, Loss: 0.2867, Val: 0.8000, Test: 0.7790\n",
      "Epoch: 152, Loss: 0.2666, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 153, Loss: 0.2294, Val: 0.8000, Test: 0.7800\n",
      "Epoch: 154, Loss: 0.2517, Val: 0.8000, Test: 0.7840\n",
      "Epoch: 155, Loss: 0.2409, Val: 0.7980, Test: 0.7850\n",
      "Epoch: 156, Loss: 0.2149, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 157, Loss: 0.2468, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 158, Loss: 0.2211, Val: 0.7960, Test: 0.7870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159, Loss: 0.2216, Val: 0.7980, Test: 0.7850\n",
      "Epoch: 160, Loss: 0.2220, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 161, Loss: 0.2258, Val: 0.8000, Test: 0.7870\n",
      "Epoch: 162, Loss: 0.2022, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 163, Loss: 0.2175, Val: 0.7820, Test: 0.7800\n",
      "Epoch: 164, Loss: 0.2061, Val: 0.7840, Test: 0.7810\n",
      "Epoch: 165, Loss: 0.2165, Val: 0.7820, Test: 0.7820\n",
      "Epoch: 166, Loss: 0.2564, Val: 0.7820, Test: 0.7840\n",
      "Epoch: 167, Loss: 0.2077, Val: 0.7900, Test: 0.7870\n",
      "Epoch: 168, Loss: 0.1866, Val: 0.7860, Test: 0.7850\n",
      "Epoch: 169, Loss: 0.2391, Val: 0.7920, Test: 0.7810\n",
      "Epoch: 170, Loss: 0.2128, Val: 0.7940, Test: 0.7790\n",
      "Epoch: 171, Loss: 0.2104, Val: 0.7920, Test: 0.7810\n",
      "Epoch: 172, Loss: 0.2003, Val: 0.7900, Test: 0.7800\n",
      "Epoch: 173, Loss: 0.2262, Val: 0.7920, Test: 0.7810\n",
      "Epoch: 174, Loss: 0.2388, Val: 0.7960, Test: 0.7810\n",
      "Epoch: 175, Loss: 0.2218, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 176, Loss: 0.2567, Val: 0.7920, Test: 0.7850\n",
      "Epoch: 177, Loss: 0.2714, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 178, Loss: 0.1924, Val: 0.7980, Test: 0.7850\n",
      "Epoch: 179, Loss: 0.1913, Val: 0.7980, Test: 0.7850\n",
      "Epoch: 180, Loss: 0.2267, Val: 0.7880, Test: 0.7850\n",
      "Epoch: 181, Loss: 0.2478, Val: 0.7880, Test: 0.7850\n",
      "Epoch: 182, Loss: 0.2068, Val: 0.7880, Test: 0.7840\n",
      "Epoch: 183, Loss: 0.1842, Val: 0.7860, Test: 0.7860\n",
      "Epoch: 184, Loss: 0.2413, Val: 0.7860, Test: 0.7830\n",
      "Epoch: 185, Loss: 0.2133, Val: 0.7880, Test: 0.7870\n",
      "Epoch: 186, Loss: 0.2414, Val: 0.7880, Test: 0.7870\n",
      "Epoch: 187, Loss: 0.1885, Val: 0.7860, Test: 0.7880\n",
      "Epoch: 188, Loss: 0.2057, Val: 0.7880, Test: 0.7910\n",
      "Epoch: 189, Loss: 0.2403, Val: 0.7920, Test: 0.7870\n",
      "Epoch: 190, Loss: 0.2155, Val: 0.7900, Test: 0.7850\n",
      "Epoch: 191, Loss: 0.2314, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 192, Loss: 0.2367, Val: 0.7960, Test: 0.7870\n",
      "Epoch: 193, Loss: 0.2370, Val: 0.7900, Test: 0.7880\n",
      "Epoch: 194, Loss: 0.1778, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 195, Loss: 0.2262, Val: 0.7940, Test: 0.7850\n",
      "Epoch: 196, Loss: 0.2051, Val: 0.7940, Test: 0.7900\n",
      "Epoch: 197, Loss: 0.2215, Val: 0.7960, Test: 0.7900\n",
      "Epoch: 198, Loss: 0.2765, Val: 0.7880, Test: 0.7820\n",
      "Epoch: 199, Loss: 0.2255, Val: 0.7900, Test: 0.7780\n",
      "Epoch: 200, Loss: 0.2480, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 201, Loss: 0.2136, Val: 0.7960, Test: 0.7760\n",
      "Epoch: 202, Loss: 0.2076, Val: 0.7960, Test: 0.7750\n",
      "Epoch: 203, Loss: 0.2500, Val: 0.7960, Test: 0.7760\n",
      "Epoch: 204, Loss: 0.2271, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 205, Loss: 0.1879, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 206, Loss: 0.2093, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 207, Loss: 0.1877, Val: 0.7840, Test: 0.7890\n",
      "Epoch: 208, Loss: 0.1742, Val: 0.7880, Test: 0.7900\n",
      "Epoch: 209, Loss: 0.2009, Val: 0.7940, Test: 0.7880\n",
      "Epoch: 210, Loss: 0.1535, Val: 0.7960, Test: 0.7890\n",
      "Epoch: 211, Loss: 0.2063, Val: 0.7960, Test: 0.7890\n",
      "Epoch: 212, Loss: 0.2135, Val: 0.7940, Test: 0.7880\n",
      "Epoch: 213, Loss: 0.2434, Val: 0.7940, Test: 0.7890\n",
      "Epoch: 214, Loss: 0.2119, Val: 0.7940, Test: 0.7890\n",
      "Epoch: 215, Loss: 0.1821, Val: 0.7900, Test: 0.7880\n",
      "Epoch: 216, Loss: 0.2004, Val: 0.7860, Test: 0.7870\n",
      "Epoch: 217, Loss: 0.2181, Val: 0.7840, Test: 0.7850\n",
      "Epoch: 218, Loss: 0.1502, Val: 0.7840, Test: 0.7850\n",
      "Epoch: 219, Loss: 0.2110, Val: 0.7900, Test: 0.7830\n",
      "Epoch: 220, Loss: 0.2150, Val: 0.7900, Test: 0.7810\n",
      "Epoch: 221, Loss: 0.1806, Val: 0.7880, Test: 0.7810\n",
      "Epoch: 222, Loss: 0.1897, Val: 0.7920, Test: 0.7840\n",
      "Epoch: 223, Loss: 0.2094, Val: 0.7860, Test: 0.7870\n",
      "Epoch: 224, Loss: 0.2332, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 225, Loss: 0.1996, Val: 0.7940, Test: 0.7850\n",
      "Epoch: 226, Loss: 0.1715, Val: 0.7980, Test: 0.7860\n",
      "Epoch: 227, Loss: 0.1816, Val: 0.7980, Test: 0.7860\n",
      "Epoch: 228, Loss: 0.2090, Val: 0.7960, Test: 0.7870\n",
      "Epoch: 229, Loss: 0.2019, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 230, Loss: 0.1972, Val: 0.7860, Test: 0.7880\n",
      "Epoch: 231, Loss: 0.1885, Val: 0.7880, Test: 0.7860\n",
      "Epoch: 232, Loss: 0.2238, Val: 0.7900, Test: 0.7850\n",
      "Epoch: 233, Loss: 0.1947, Val: 0.7980, Test: 0.7840\n",
      "Epoch: 234, Loss: 0.2214, Val: 0.7960, Test: 0.7830\n",
      "Epoch: 235, Loss: 0.1806, Val: 0.8040, Test: 0.7820\n",
      "Epoch: 236, Loss: 0.2671, Val: 0.7960, Test: 0.7860\n",
      "Epoch: 237, Loss: 0.1613, Val: 0.7940, Test: 0.7900\n",
      "Epoch: 238, Loss: 0.1845, Val: 0.7940, Test: 0.7900\n",
      "Epoch: 239, Loss: 0.1770, Val: 0.7960, Test: 0.7900\n",
      "Epoch: 240, Loss: 0.1936, Val: 0.7980, Test: 0.7920\n",
      "Epoch: 241, Loss: 0.2289, Val: 0.7940, Test: 0.7870\n",
      "Epoch: 242, Loss: 0.1943, Val: 0.7960, Test: 0.7880\n",
      "Epoch: 243, Loss: 0.2182, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 244, Loss: 0.2252, Val: 0.7880, Test: 0.7850\n",
      "Epoch: 245, Loss: 0.1853, Val: 0.7800, Test: 0.7900\n",
      "Epoch: 246, Loss: 0.1721, Val: 0.7880, Test: 0.7870\n",
      "Epoch: 247, Loss: 0.1981, Val: 0.7940, Test: 0.7840\n",
      "Epoch: 248, Loss: 0.1864, Val: 0.7940, Test: 0.7820\n",
      "Epoch: 249, Loss: 0.1764, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 250, Loss: 0.1682, Val: 0.7920, Test: 0.7820\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GAT(hidden_channels=8,heads=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x,data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 251):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    test_acc = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZbnHXjRJQFF",
    "outputId": "3248c279-cc1d-4329-ccd7-cc9f6e0da6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7820\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(data.test_mask)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pubmed Document Classification using GraphDL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
